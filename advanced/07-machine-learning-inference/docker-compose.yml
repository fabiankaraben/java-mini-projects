services:
  ml-inference:
    build: .
    ports:
      - "8080:8080"
    environment:
      - JAVA_OPTS=-Xmx1g
    volumes:
      - ~/.djl.cache:/root/.djl.cache # Cache models on host
